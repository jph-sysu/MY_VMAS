import torch
import numpy as np
import vmas
import matplotlib.pyplot as plt
from collections import deque
import time
import json
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class BalanceTask:
    """ç›´æ¥å®ç°Balanceä»»åŠ¡çš„è®­ç»ƒï¼Œä¸ä¾èµ–RLlib"""
    
    def __init__(self, num_agents=3, device="cpu"):
        self.num_agents = num_agents
        self.device = device
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = vmas.make_env(
            scenario="balance",
            num_envs=1,
            device=self.device,
            continuous_actions=True,
            max_steps=200,
            wrapper=None,
            dict_spaces=True,
        )
        
        # è·å–ç¯å¢ƒä¿¡æ¯
        self.agent_ids = list(self.env.observation_space.spaces.keys())
        self.obs_dim = self.env.observation_space[self.agent_ids[0]].shape[0]  # 16
        self.action_dim = self.env.action_space[self.agent_ids[0]].shape[0]    # 2
        
        print(f"Balanceä»»åŠ¡åˆå§‹åŒ–å®Œæˆ")
        print(f"æ™ºèƒ½ä½“æ•°é‡: {self.num_agents}")
        print(f"è§‚æµ‹ç»´åº¦: {self.obs_dim}")
        print(f"åŠ¨ä½œç»´åº¦: {self.action_dim}")
        
        # é‡ç½®ç¯å¢ƒ
        self.reset()
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        obs_dict = self.env.reset()
        # è½¬æ¢ä¸ºnumpyå¹¶ç§»é™¤æ‰¹é‡ç»´åº¦
        obs = {}
        for agent_id in self.agent_ids:
            agent_obs = obs_dict[agent_id]
            if isinstance(agent_obs, torch.Tensor):
                obs[agent_id] = agent_obs[0].cpu().numpy()
            else:
                obs[agent_id] = np.array(agent_obs)
        return obs
    
    def step(self, actions):
        """æ‰§è¡Œä¸€æ­¥"""
        # å‡†å¤‡åŠ¨ä½œå¼ é‡
        action_tensors = {}
        for agent_id in self.agent_ids:
            action = actions[agent_id]
            # ç¡®ä¿åŠ¨ä½œåœ¨[-1, 1]èŒƒå›´å†…
            action = np.clip(action, -1.0, 1.0)
            action_tensors[agent_id] = torch.tensor(action).unsqueeze(0).float()
        
        # æ‰§è¡Œä¸€æ­¥
        obs_dict, rewards, dones, infos = self.env.step(action_tensors)
        
        # è½¬æ¢è¿”å›å€¼
        obs = {}
        for agent_id in self.agent_ids:
            agent_obs = obs_dict[agent_id]
            if isinstance(agent_obs, torch.Tensor):
                obs[agent_id] = agent_obs[0].cpu().numpy()
            else:
                obs[agent_id] = np.array(agent_obs)
        
        # è®¡ç®—æ€»å¥–åŠ±
        if isinstance(rewards, dict):
            total_reward = sum([float(rewards[agent_id].item()) for agent_id in self.agent_ids])
        else:
            total_reward = float(rewards.item()) * self.num_agents
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        if isinstance(dones, dict):
            done = any([bool(dones[agent_id].item()) for agent_id in self.agent_ids])
        else:
            done = bool(dones.item())
        
        return obs, total_reward, done, {}

class IPPOAgent:
    """ç‹¬ç«‹PPOæ™ºèƒ½ä½“"""
    
    def __init__(self, obs_dim, action_dim, agent_id, learning_rate=3e-4, gamma=0.99):
        self.agent_id = agent_id
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.lr = learning_rate
        
        # ç­–ç•¥ç½‘ç»œï¼ˆactorï¼‰
        self.policy_net = torch.nn.Sequential(
            torch.nn.Linear(obs_dim, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, action_dim),
        )
        
        # ä»·å€¼ç½‘ç»œï¼ˆcriticï¼‰
        self.value_net = torch.nn.Sequential(
            torch.nn.Linear(obs_dim, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 64),
            torch.nn.Tanh(),
            torch.nn.Linear(64, 1),
        )
        
        # ä¼˜åŒ–å™¨
        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=learning_rate)
        
        # ç»éªŒç¼“å†²åŒº
        self.memory = []
        
        # ç”¨äºè®¡ç®—åŠ¨ä½œæ ‡å‡†å·®
        self.log_std = torch.nn.Parameter(torch.zeros(1, action_dim))
        
    def get_action(self, obs, deterministic=False):
        """æ ¹æ®è§‚æµ‹é€‰æ‹©åŠ¨ä½œ"""
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
        
        # è·å–å‡å€¼
        action_mean = self.policy_net(obs_tensor)
        
        if deterministic:
            return action_mean.detach().numpy().squeeze(0), None
        
        # æ·»åŠ æ¢ç´¢å™ªå£°
        action_std = torch.exp(self.log_std)
        dist = torch.distributions.Normal(action_mean, action_std)
        action = dist.sample()
        
        # è®¡ç®—logæ¦‚ç‡
        log_prob = dist.log_prob(action).sum(dim=-1)
        
        return action.detach().numpy().squeeze(0), log_prob
    
    def compute_value(self, obs):
        """è®¡ç®—çŠ¶æ€ä»·å€¼"""
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
        return self.value_net(obs_tensor).squeeze()
    
    def store_transition(self, obs, action, log_prob, reward, next_obs, done):
        """å­˜å‚¨è½¬ç§»"""
        self.memory.append({
            'obs': obs,
            'action': action,
            'log_prob': log_prob.detach() if log_prob is not None else torch.tensor(0.0),
            'reward': reward,
            'next_obs': next_obs,
            'done': done
        })
    
    def update(self, clip_param=0.2, value_coef=0.5, entropy_coef=0.01):
        """æ›´æ–°ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ"""
        if len(self.memory) < 32:  # æœ€å°æ‰¹é‡å¤§å°
            return 0, 0
        
        # è®¡ç®—æŠ˜æ‰£å›æŠ¥å’Œä¼˜åŠ¿
        returns = []
        advantages = []
        
        # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„å›æŠ¥
        R = 0
        for t in reversed(range(len(self.memory))):
            transition = self.memory[t]
            reward = transition['reward']
            done = transition['done']
            
            R = reward + self.gamma * R * (1 - done)
            returns.insert(0, R)
        
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # æ ‡å‡†åŒ–
        
        # è®¡ç®—ä¼˜åŠ¿
        for i, transition in enumerate(self.memory):
            obs = transition['obs']
            value = self.compute_value(obs)
            advantage = returns[i] - value.detach()
            advantages.append(advantage)
        
        advantages = torch.FloatTensor(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å‡†å¤‡æ‰¹é‡æ•°æ® - ä¿®å¤å¼ é‡åˆ›å»ºè­¦å‘Š
        obs_list = [t['obs'] for t in self.memory]
        observations = torch.FloatTensor(np.array(obs_list))
        
        action_list = [t['action'] for t in self.memory]
        actions = torch.FloatTensor(np.array(action_list))
        
        old_log_probs = torch.stack([t['log_prob'] for t in self.memory])
        
        # è®¡ç®—æ–°logæ¦‚ç‡
        action_means = self.policy_net(observations)
        action_stds = torch.exp(self.log_std).expand_as(action_means)
        dist = torch.distributions.Normal(action_means, action_stds)
        new_log_probs = dist.log_prob(actions).sum(dim=-1)
        
        # è®¡ç®—æ¦‚ç‡æ¯”
        ratio = torch.exp(new_log_probs - old_log_probs)
        
        # PPOæŸå¤±
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # ä»·å€¼æŸå¤±
        values = self.value_net(observations).squeeze()
        value_loss = 0.5 * torch.nn.functional.mse_loss(values, returns)
        
        # ç†µæ­£åˆ™åŒ–
        entropy = dist.entropy().mean()
        
        # æ€»æŸå¤±
        total_policy_loss = policy_loss - entropy_coef * entropy
        total_value_loss = value_coef * value_loss
        
        # æ›´æ–°ç­–ç•¥ç½‘ç»œ
        self.policy_optimizer.zero_grad()
        total_policy_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
        self.policy_optimizer.step()
        
        # æ›´æ–°ä»·å€¼ç½‘ç»œ
        self.value_optimizer.zero_grad()
        total_value_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
        self.value_optimizer.step()
        
        # æ¸…ç©ºå†…å­˜
        self.memory.clear()
        
        return policy_loss.item(), value_loss.item()

class CPPOAgent:
    """é›†ä¸­å¼PPOæ™ºèƒ½ä½“ï¼ˆæ‰€æœ‰æ™ºèƒ½ä½“å…±äº«ç­–ç•¥ï¼‰"""
    
    def __init__(self, num_agents, obs_dim, action_dim, learning_rate=3e-4, gamma=0.99):
        self.num_agents = num_agents
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.lr = learning_rate
        
        # é›†ä¸­å¼ç­–ç•¥ç½‘ç»œï¼ˆè¾“å…¥æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ï¼‰
        self.policy_net = torch.nn.Sequential(
            torch.nn.Linear(obs_dim * num_agents, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, action_dim * num_agents),
        )
        
        # é›†ä¸­å¼ä»·å€¼ç½‘ç»œ
        self.value_net = torch.nn.Sequential(
            torch.nn.Linear(obs_dim * num_agents, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 1),
        )
        
        # ä¼˜åŒ–å™¨
        self.policy_optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=learning_rate)
        
        # ç»éªŒç¼“å†²åŒº
        self.memory = []
        
        # åŠ¨ä½œæ ‡å‡†å·® - ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“å•ç‹¬è®¾ç½®
        self.log_std = torch.nn.Parameter(torch.zeros(num_agents, action_dim))
    
    def get_actions(self, obs_dict, deterministic=False):
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ"""
        # æ‹¼æ¥æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹
        obs_list = []
        for i in range(self.num_agents):
            agent_id = f'agent_{i}'
            if agent_id in obs_dict:
                obs_list.append(obs_dict[agent_id])
            else:
                # å¦‚æœæ²¡æœ‰è¯¥æ™ºèƒ½ä½“çš„è§‚æµ‹ï¼Œä½¿ç”¨é›¶å‘é‡
                obs_list.append(np.zeros(self.obs_dim))
        
        joint_obs = np.concatenate(obs_list)
        obs_tensor = torch.FloatTensor(joint_obs).unsqueeze(0)
        
        # è·å–æ‰€æœ‰åŠ¨ä½œçš„å‡å€¼
        action_means = self.policy_net(obs_tensor)
        # é‡å¡‘ä¸º (batch_size, num_agents, action_dim)
        action_means = action_means.view(-1, self.num_agents, self.action_dim)
        
        actions = {}
        log_probs_list = []
        
        if deterministic:
            for i in range(self.num_agents):
                action = action_means[0, i].detach().numpy()
                actions[f'agent_{i}'] = action
            return actions, None
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“é‡‡æ ·åŠ¨ä½œ
        for i in range(self.num_agents):
            # è·å–è¯¥æ™ºèƒ½ä½“çš„å‡å€¼å’Œæ ‡å‡†å·®
            agent_mean = action_means[0, i]
            agent_log_std = self.log_std[i]
            agent_std = torch.exp(agent_log_std)
            
            # åˆ›å»ºåˆ†å¸ƒå¹¶é‡‡æ ·
            dist = torch.distributions.Normal(agent_mean, agent_std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum()
            
            # å­˜å‚¨åŠ¨ä½œå’Œlogæ¦‚ç‡
            actions[f'agent_{i}'] = action.detach().numpy()
            log_probs_list.append(log_prob)
        
        log_probs = torch.stack(log_probs_list) if log_probs_list else None
        return actions, log_probs
    
    def compute_value(self, obs_dict):
        """è®¡ç®—è”åˆçŠ¶æ€ä»·å€¼"""
        # æ‹¼æ¥æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹
        obs_list = []
        for i in range(self.num_agents):
            agent_id = f'agent_{i}'
            if agent_id in obs_dict:
                obs_list.append(obs_dict[agent_id])
            else:
                obs_list.append(np.zeros(self.obs_dim))
        
        joint_obs = np.concatenate(obs_list)
        obs_tensor = torch.FloatTensor(joint_obs).unsqueeze(0)
        return self.value_net(obs_tensor).squeeze()
    
    def store_transition(self, obs_dict, actions, log_probs, reward, next_obs_dict, done):
        """å­˜å‚¨è½¬ç§»"""
        if log_probs is not None:
            log_probs = log_probs.detach()
        self.memory.append({
            'obs_dict': obs_dict,
            'actions': actions,
            'log_probs': log_probs,
            'reward': reward,
            'next_obs_dict': next_obs_dict,
            'done': done
        })
    
    def update(self, clip_param=0.2, value_coef=0.5, entropy_coef=0.01):
        """æ›´æ–°ç½‘ç»œ"""
        if len(self.memory) < 32:
            return 0, 0
        
        # è®¡ç®—æŠ˜æ‰£å›æŠ¥
        returns = []
        R = 0
        for t in reversed(range(len(self.memory))):
            transition = self.memory[t]
            reward = transition['reward']
            done = transition['done']
            R = reward + self.gamma * R * (1 - done)
            returns.insert(0, R)
        
        returns = torch.FloatTensor(returns)
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)
        
        # è®¡ç®—ä¼˜åŠ¿
        advantages = []
        for i, transition in enumerate(self.memory):
            obs_dict = transition['obs_dict']
            value = self.compute_value(obs_dict)
            advantage = returns[i] - value.detach()
            advantages.append(advantage)
        
        advantages = torch.FloatTensor(advantages)
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # å‡†å¤‡æ•°æ®
        old_log_probs_sum = torch.stack([t['log_probs'].sum() for t in self.memory])
        
        # è®¡ç®—æ–°logæ¦‚ç‡
        observations = []
        actions_list = []
        
        for transition in self.memory:
            obs_dict = transition['obs_dict']
            # æ‹¼æ¥è§‚æµ‹
            obs_concat = []
            for i in range(self.num_agents):
                agent_id = f'agent_{i}'
                if agent_id in obs_dict:
                    obs_concat.append(obs_dict[agent_id])
                else:
                    obs_concat.append(np.zeros(self.obs_dim))
            observations.append(np.concatenate(obs_concat))
            
            # æ‹¼æ¥åŠ¨ä½œ
            actions = transition['actions']
            action_concat = []
            for i in range(self.num_agents):
                agent_id = f'agent_{i}'
                if agent_id in actions:
                    action_concat.append(actions[agent_id])
                else:
                    action_concat.append(np.zeros(self.action_dim))
            actions_list.append(np.concatenate(action_concat))
        
        observations = torch.FloatTensor(np.array(observations))
        actions_tensor = torch.FloatTensor(np.array(actions_list))
        
        # å‰å‘ä¼ æ’­è·å–åŠ¨ä½œå‡å€¼
        action_means_all = self.policy_net(observations)
        action_means_all = action_means_all.view(-1, self.num_agents, self.action_dim)
        actions_tensor = actions_tensor.view(-1, self.num_agents, self.action_dim)
        
        # è®¡ç®—æ–°logæ¦‚ç‡
        new_log_probs_list = []
        for i in range(len(observations)):
            log_prob_sum = 0
            for j in range(self.num_agents):
                agent_mean = action_means_all[i, j]
                agent_log_std = self.log_std[j]
                agent_std = torch.exp(agent_log_std)
                
                dist = torch.distributions.Normal(agent_mean, agent_std)
                log_prob = dist.log_prob(actions_tensor[i, j]).sum()
                log_prob_sum += log_prob
            new_log_probs_list.append(log_prob_sum)
        
        new_log_probs = torch.stack(new_log_probs_list)
        
        # PPOæŸå¤±
        ratio = torch.exp(new_log_probs - old_log_probs_sum)
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantages
        policy_loss = -torch.min(surr1, surr2).mean()
        
        # ä»·å€¼æŸå¤±
        values = self.value_net(observations).squeeze()
        value_loss = 0.5 * torch.nn.functional.mse_loss(values, returns)
        
        # ç†µæ­£åˆ™åŒ–
        entropy = 0
        for i in range(len(observations)):
            for j in range(self.num_agents):
                agent_mean = action_means_all[i, j]
                agent_log_std = self.log_std[j]
                agent_std = torch.exp(agent_log_std)
                
                dist = torch.distributions.Normal(agent_mean, agent_std)
                entropy += dist.entropy().mean()
        
        entropy = entropy / (len(observations) * self.num_agents)
        
        # æ€»æŸå¤±
        total_policy_loss = policy_loss - entropy_coef * entropy
        total_value_loss = value_coef * value_loss
        
        # æ›´æ–° - ä¿®å¤åå‘ä¼ æ’­é—®é¢˜
        self.policy_optimizer.zero_grad()
        self.value_optimizer.zero_grad()
        
        # åˆ†åˆ«è®¡ç®—æ¢¯åº¦
        total_policy_loss.backward(retain_graph=True)
        total_value_loss.backward()
        
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
        
        self.policy_optimizer.step()
        self.value_optimizer.step()
        
        # æ¸…ç©ºå†…å­˜
        self.memory.clear()
        
        return policy_loss.item(), value_loss.item()

class MAPPOAgent:
    """MAPPOæ™ºèƒ½ä½“ï¼ˆé›†ä¸­å¼Criticï¼Œåˆ†å¸ƒå¼Actorï¼‰"""
    
    def __init__(self, num_agents, obs_dim, action_dim, learning_rate=3e-4, gamma=0.99):
        self.num_agents = num_agents
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.lr = learning_rate
        
        # ä¸ºæ¯ä¸ªæ™ºèƒ½ä½“åˆ›å»ºç‹¬ç«‹çš„ç­–ç•¥ç½‘ç»œ
        self.policy_nets = torch.nn.ModuleList([
            torch.nn.Sequential(
                torch.nn.Linear(obs_dim, 64),
                torch.nn.Tanh(),
                torch.nn.Linear(64, 64),
                torch.nn.Tanh(),
                torch.nn.Linear(64, action_dim),
            ) for _ in range(num_agents)
        ])
        
        # é›†ä¸­å¼ä»·å€¼ç½‘ç»œï¼ˆè¾“å…¥æ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ï¼‰
        self.value_net = torch.nn.Sequential(
            torch.nn.Linear(obs_dim * num_agents, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 128),
            torch.nn.Tanh(),
            torch.nn.Linear(128, 1),
        )
        
        # ä¼˜åŒ–å™¨
        self.policy_optimizers = [torch.optim.Adam(net.parameters(), lr=learning_rate) 
                                 for net in self.policy_nets]
        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=learning_rate)
        
        # ç»éªŒç¼“å†²åŒº
        self.memories = [[] for _ in range(num_agents)]
        
        # åŠ¨ä½œæ ‡å‡†å·®
        self.log_stds = torch.nn.ParameterList([
            torch.nn.Parameter(torch.zeros(1, action_dim)) for _ in range(num_agents)
        ])
    
    def get_actions(self, obs_dict, deterministic=False):
        """è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ"""
        actions = {}
        log_probs = []
        
        for i in range(self.num_agents):
            agent_id = f'agent_{i}'
            obs = obs_dict[agent_id]
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            
            # è·å–åŠ¨ä½œå‡å€¼
            action_mean = self.policy_nets[i](obs_tensor)
            
            if deterministic:
                actions[agent_id] = action_mean.detach().numpy().squeeze(0)
                continue
            
            # é‡‡æ ·åŠ¨ä½œ
            action_std = torch.exp(self.log_stds[i])
            dist = torch.distributions.Normal(action_mean, action_std)
            action = dist.sample()
            log_prob = dist.log_prob(action).sum(dim=-1)
            
            actions[agent_id] = action.detach().numpy().squeeze(0)
            log_probs.append(log_prob)
        
        if deterministic:
            return actions, None
        
        return actions, torch.stack(log_probs)
    
    def compute_value(self, obs_dict):
        """è®¡ç®—è”åˆçŠ¶æ€ä»·å€¼"""
        obs_list = [obs_dict[f'agent_{i}'] for i in range(self.num_agents)]
        joint_obs = np.concatenate(obs_list)
        obs_tensor = torch.FloatTensor(joint_obs).unsqueeze(0)
        return self.value_net(obs_tensor).squeeze()
    
    def store_transition(self, agent_idx, obs, action, log_prob, reward, next_obs, done):
        """ä¸ºå•ä¸ªæ™ºèƒ½ä½“å­˜å‚¨è½¬ç§»"""
        self.memories[agent_idx].append({
            'obs': obs,
            'action': action,
            'log_prob': log_prob.detach() if log_prob is not None else torch.tensor(0.0),
            'reward': reward,
            'next_obs': next_obs,
            'done': done
        })
    
    def update(self, clip_param=0.2, value_coef=0.5, entropy_coef=0.01):
        """æ›´æ–°æ‰€æœ‰ç½‘ç»œ"""
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        if not all(len(memory) >= 32 for memory in self.memories):
            return 0, 0
        
        # å‡è®¾æ‰€æœ‰æ™ºèƒ½ä½“çš„ç»éªŒé•¿åº¦ç›¸åŒ
        episode_length = len(self.memories[0])
        
        # è®¡ç®—æŠ˜æ‰£å›æŠ¥ï¼ˆä½¿ç”¨é›†ä¸­å¼Criticï¼‰
        joint_returns = []
        R = 0
        
        for t in reversed(range(episode_length)):
            # è®¡ç®—æ—¶é—´æ­¥tçš„æ€»å¥–åŠ±
            total_reward = sum(self.memories[i][t]['reward'] for i in range(self.num_agents))
            done = any(self.memories[i][t]['done'] for i in range(self.num_agents))
            
            R = total_reward + self.gamma * R * (1 - int(done))
            joint_returns.insert(0, R)
        
        joint_returns = torch.FloatTensor(joint_returns)
        joint_returns = (joint_returns - joint_returns.mean()) / (joint_returns.std() + 1e-8)
        
        # ä¸ºæ¯ä¸ªæ—¶é—´æ­¥è®¡ç®—é›†ä¸­å¼ä»·å€¼
        joint_values = []
        for t in range(episode_length):
            obs_dict = {}
            for i in range(self.num_agents):
                obs_dict[f'agent_{i}'] = self.memories[i][t]['obs']
            value = self.compute_value(obs_dict)
            joint_values.append(value)
        
        joint_values = torch.stack(joint_values)
        
        # è®¡ç®—ä¼˜åŠ¿ï¼Œå¹¶åˆ†ç¦»è®¡ç®—å›¾
        advantages = joint_returns - joint_values.detach()
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # æ›´æ–°ä»·å€¼ç½‘ç»œ
        value_loss = 0.5 * torch.nn.functional.mse_loss(joint_values, joint_returns)
        
        self.value_optimizer.zero_grad()
        value_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)
        self.value_optimizer.step()
        
        # æ›´æ–°æ¯ä¸ªæ™ºèƒ½ä½“çš„ç­–ç•¥ç½‘ç»œ
        policy_losses = []
        
        # åˆ†ç¦»ä¼˜åŠ¿å€¼ï¼Œé¿å…ä¸ä»·å€¼ç½‘ç»œçš„è®¡ç®—å›¾å…³è”
        advantages = advantages.detach()
        
        for i in range(self.num_agents):
            if len(self.memories[i]) >= 32:
                # æå–æ™ºèƒ½ä½“içš„ç»éªŒ
                memory = self.memories[i]
                
                # å‡†å¤‡æ•°æ® - ä¿®å¤å¼ é‡åˆ›å»ºè­¦å‘Š
                obs_list = [t['obs'] for t in memory]
                observations = torch.FloatTensor(np.array(obs_list))
                
                action_list = [t['action'] for t in memory]
                actions = torch.FloatTensor(np.array(action_list))
                
                old_log_probs = torch.stack([t['log_prob'] for t in memory])
                
                # å‰å‘ä¼ æ’­
                action_means = self.policy_nets[i](observations)
                action_std = torch.exp(self.log_stds[i])
                dist = torch.distributions.Normal(action_means, action_std)
                new_log_probs = dist.log_prob(actions).sum(dim=-1)
                
                # PPOæŸå¤±
                ratio = torch.exp(new_log_probs - old_log_probs)
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - clip_param, 1 + clip_param) * advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # ç†µæ­£åˆ™åŒ–
                entropy = dist.entropy().mean()
                
                # æ€»æŸå¤±
                total_loss = policy_loss - entropy_coef * entropy
                
                # æ›´æ–°ç­–ç•¥ç½‘ç»œ
                self.policy_optimizers[i].zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.policy_nets[i].parameters(), 0.5)
                self.policy_optimizers[i].step()
                
                policy_losses.append(policy_loss.item())
        
        # æ¸…ç©ºæ‰€æœ‰å†…å­˜
        for memory in self.memories:
            memory.clear()
        
        return np.mean(policy_losses) if policy_losses else 0, value_loss.item()

def train_algorithm(algorithm_name, num_episodes=100):
    """è®­ç»ƒæŒ‡å®šç®—æ³• - åªåœ¨è®­ç»ƒå®Œæˆåä¿å­˜ä¸€æ¬¡ç»“æœ"""
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒ {algorithm_name} ç®—æ³•")
    print(f"{'='*60}")
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    task = BalanceTask()
    num_agents = task.num_agents
    obs_dim = task.obs_dim
    action_dim = task.action_dim
    
    # æ ¹æ®ç®—æ³•é€‰æ‹©æ™ºèƒ½ä½“ç±»å‹
    if algorithm_name == "IPPO":
        # æ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹å­¦ä¹ 
        agents = [IPPOAgent(obs_dim, action_dim, i) for i in range(num_agents)]
        agent_type = "independent"
    elif algorithm_name == "CPPO":
        # æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«ä¸€ä¸ªç­–ç•¥
        agent = CPPOAgent(num_agents, obs_dim, action_dim)
        agent_type = "centralized"
    elif algorithm_name == "MAPPO":
        # MAPPOï¼šåˆ†å¸ƒå¼Actorï¼Œé›†ä¸­å¼Critic
        agent = MAPPOAgent(num_agents, obs_dim, action_dim)
        agent_type = "mappo"
    else:
        raise ValueError(f"æœªçŸ¥ç®—æ³•: {algorithm_name}")
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    avg_rewards = []
    loss_history = {'policy': [], 'value': []}
    
    # è®­ç»ƒå¾ªç¯
    start_time = time.time()
    for episode in range(num_episodes):
        # é‡ç½®ç¯å¢ƒ
        obs = task.reset()
        total_reward = 0
        done = False
        step_count = 0
        
        # å­˜å‚¨æ¯ä¸ªæ­¥éª¤çš„ç»éªŒ
        episode_experience = []
        
        while not done and step_count < 200:
            # æ ¹æ®ç®—æ³•ç±»å‹é€‰æ‹©åŠ¨ä½œ
            if algorithm_name == "IPPO":
                actions = {}
                log_probs = []
                
                for i, agent in enumerate(agents):
                    agent_id = f'agent_{i}'
                    action, log_prob = agent.get_action(obs[agent_id])
                    actions[agent_id] = action
                    log_probs.append(log_prob)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, _ = task.step(actions)
                
                # å­˜å‚¨ç»éªŒï¼ˆç‹¬ç«‹ï¼‰
                for i, agent in enumerate(agents):
                    agent_id = f'agent_{i}'
                    agent.store_transition(
                        obs[agent_id], actions[agent_id], log_probs[i],
                        reward/num_agents, next_obs[agent_id], done
                    )
                
                # å®šæœŸæ›´æ–°
                if step_count % 32 == 0:
                    policy_losses, value_losses = [], []
                    for agent in agents:
                        pl, vl = agent.update()
                        if pl != 0:
                            policy_losses.append(pl)
                            value_losses.append(vl)
                    
                    if policy_losses:
                        loss_history['policy'].append(np.mean(policy_losses))
                        loss_history['value'].append(np.mean(value_losses))
            
            elif algorithm_name == "CPPO":
                # é›†ä¸­å¼PPO
                actions, log_probs = agent.get_actions(obs)
                
                # æ£€æŸ¥åŠ¨ä½œå½¢çŠ¶
                for agent_id, action in actions.items():
                    if action.shape != (action_dim,):
                        # å¦‚æœå½¢çŠ¶ä¸æ­£ç¡®ï¼Œä¿®æ­£ä¸ºé›¶åŠ¨ä½œ
                        actions[agent_id] = np.zeros(action_dim)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, _ = task.step(actions)
                
                # å­˜å‚¨ç»éªŒï¼ˆé›†ä¸­ï¼‰
                if log_probs is not None:
                    agent.store_transition(obs, actions, log_probs, reward, next_obs, done)
                
                # å®šæœŸæ›´æ–°
                if step_count % 32 == 0:
                    pl, vl = agent.update()
                    if pl != 0:
                        loss_history['policy'].append(pl)
                        loss_history['value'].append(vl)
            
            elif algorithm_name == "MAPPO":
                # MAPPO
                actions, log_probs = agent.get_actions(obs)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, _ = task.step(actions)
                
                # å­˜å‚¨ç»éªŒï¼ˆåˆ†å¸ƒå¼ï¼‰
                for i in range(num_agents):
                    agent_id = f'agent_{i}'
                    agent.store_transition(
                        i, obs[agent_id], actions[agent_id], log_probs[i],
                        reward/num_agents, next_obs[agent_id], done
                    )
                
                # å®šæœŸæ›´æ–°
                if step_count % 32 == 0:
                    pl, vl = agent.update()
                    if pl != 0:
                        loss_history['policy'].append(pl)
                        loss_history['value'].append(vl)
            
            # æ›´æ–°çŠ¶æ€
            obs = next_obs
            total_reward += reward
            step_count += 1
        
        # è®°å½•å¥–åŠ±
        episode_rewards.append(total_reward)
        
        # è®¡ç®—æ»‘åŠ¨å¹³å‡å¥–åŠ±
        if len(episode_rewards) >= 10:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_rewards.append(avg_reward)
        else:
            avg_reward = np.mean(episode_rewards)
            avg_rewards.append(avg_reward)
        
        # æ‰“å°è¿›åº¦
        if (episode + 1) % 10 == 0:
            print(f"Episode {episode+1}/{num_episodes}: "
                  f"Reward = {total_reward:.2f}, "
                  f"Avg Reward = {avg_reward:.2f}, "
                  f"Steps = {step_count}")
    
    # è®­ç»ƒå®Œæˆåä¿å­˜ä¸€æ¬¡ç»“æœ
    end_time = time.time()
    training_time = end_time - start_time
    
    # ä¿å­˜ç»“æœ
    save_results(algorithm_name, episode_rewards, avg_rewards, loss_history, num_episodes, training_time)
    
    return episode_rewards, avg_rewards, loss_history

def save_results(algorithm_name, episode_rewards, avg_rewards, loss_history, num_episodes, training_time):
    """ä¿å­˜è®­ç»ƒç»“æœ - æ¯ä¸ªç®—æ³•åªä¿å­˜ä¸€ä¸ªæ–‡ä»¶"""
    os.makedirs(f"./results/{algorithm_name}", exist_ok=True)
    
    # ä¿å­˜æ•°æ®åˆ°å•ä¸ªJSONæ–‡ä»¶
    results = {
        'algorithm': algorithm_name,
        'num_episodes': num_episodes,
        'training_time_seconds': training_time,
        'episode_rewards': episode_rewards,
        'avg_rewards': avg_rewards,
        'loss_history': loss_history,
        'final_avg_reward': np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards),
        'best_reward': max(episode_rewards),
        'worst_reward': min(episode_rewards),
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # ä¿å­˜åˆ°å•ä¸ªJSONæ–‡ä»¶
    with open(f"./results/{algorithm_name}/results.json", 'w') as f:
        json.dump(results, f, indent=2)
    
    # ç»˜åˆ¶å›¾è¡¨
    plt.figure(figsize=(15, 10))
    
    # 1. å¥–åŠ±æ›²çº¿
    plt.subplot(2, 3, 1)
    plt.plot(episode_rewards, alpha=0.6, label='Episode Reward', color='blue')
    if avg_rewards:
        plt.plot(avg_rewards, 'r-', linewidth=2, label='Average Reward (window=10)')
    plt.xlabel('Episode')
    plt.ylabel('Reward')
    plt.title(f'{algorithm_name} - Training Rewards\nFinal Avg: {results["final_avg_reward"]:.2f}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. æŸå¤±æ›²çº¿
    plt.subplot(2, 3, 2)
    if loss_history['policy']:
        plt.plot(loss_history['policy'], label='Policy Loss', color='green')
    if loss_history['value']:
        plt.plot(loss_history['value'], label='Value Loss', color='orange')
    plt.xlabel('Update Step')
    plt.ylabel('Loss')
    plt.title(f'{algorithm_name} - Training Losses')
    if loss_history['policy'] or loss_history['value']:
        plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. å¥–åŠ±åˆ†å¸ƒç›´æ–¹å›¾
    plt.subplot(2, 3, 3)
    if episode_rewards and len(episode_rewards) >= 20:
        last_n = min(100, len(episode_rewards))
        plt.hist(episode_rewards[-last_n:], bins=20, alpha=0.7, edgecolor='black', color='purple')
        plt.xlabel('Reward')
        plt.ylabel('Frequency')
        plt.title(f'Reward Distribution (last {last_n} episodes)')
    
    # 4. æ»‘åŠ¨å¹³å‡æ›²çº¿ï¼ˆæ›´å¹³æ»‘ï¼‰
    plt.subplot(2, 3, 4)
    if len(avg_rewards) > 20:
        window = 20
        smooth_avg = np.convolve(avg_rewards, np.ones(window)/window, mode='valid')
        plt.plot(range(window-1, len(avg_rewards)), smooth_avg, 'g-', linewidth=2)
        plt.xlabel('Episode')
        plt.ylabel('Smoothed Average Reward')
        plt.title(f'Smoothed Average (window={window})')
        plt.grid(True, alpha=0.3)
    
    # 5. è®­ç»ƒè¿›åº¦åˆ†æ
    plt.subplot(2, 3, 5)
    if len(episode_rewards) >= 4:
        # å°†è®­ç»ƒåˆ†ä¸º4ä¸ªé˜¶æ®µ
        quarter = len(episode_rewards) // 4
        quarter_avgs = []
        quarter_labels = ['0-25%', '25-50%', '50-75%', '75-100%']
        
        for i in range(4):
            start_idx = i * quarter
            end_idx = (i + 1) * quarter if i < 3 else len(episode_rewards)
            quarter_avg = np.mean(episode_rewards[start_idx:end_idx])
            quarter_avgs.append(quarter_avg)
        
        bars = plt.bar(quarter_labels, quarter_avgs, alpha=0.7, color=['red', 'orange', 'yellow', 'green'])
        plt.xlabel('Training Phase')
        plt.ylabel('Average Reward')
        plt.title('Learning Progress by Phase')
        
        for bar, value in zip(bars, quarter_avgs):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{value:.2f}', ha='center', va='bottom')
    
    # 6. è®­ç»ƒä¿¡æ¯æ€»ç»“
    plt.subplot(2, 3, 6)
    plt.axis('off')
    info_text = (
        f"Algorithm: {algorithm_name}\n"
        f"Total Episodes: {num_episodes}\n"
        f"Training Time: {training_time:.2f}s\n"
        f"Final Avg Reward: {results['final_avg_reward']:.2f}\n"
        f"Best Reward: {results['best_reward']:.2f}\n"
        f"Worst Reward: {results['worst_reward']:.2f}\n"
        f"Last 10 Avg: {np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 'N/A'}\n"
        f"Timestamp: {results['timestamp']}"
    )
    plt.text(0.1, 0.5, info_text, fontsize=10, verticalalignment='center',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.suptitle(f'{algorithm_name} Algorithm Training Results', fontsize=16, fontweight='bold')
    plt.tight_layout()
    
    # ä¿å­˜å•ä¸ªPNGæ–‡ä»¶
    plt.savefig(f"./results/{algorithm_name}/training_results.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"\nè®­ç»ƒå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ°: ./results/{algorithm_name}/")
    print(f"  - JSONæ–‡ä»¶: results.json")
    print(f"  - å›¾è¡¨æ–‡ä»¶: training_results.png")
    print(f"  - è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
    print(f"  - æœ€ç»ˆå¹³å‡å¥–åŠ±: {results['final_avg_reward']:.2f}")

def analyze_results(algorithms=None):
    """åˆ†æå¤šç§ç®—æ³•çš„ç»“æœ"""
    if algorithms is None:
        algorithms = ["IPPO", "CPPO", "MAPPO"]
    
    print("\n" + "="*80)
    print("åˆ†æç®—æ³•è®­ç»ƒç»“æœ")
    print("="*80)
    
    results = {}
    
    # åŠ è½½å„ä¸ªç®—æ³•çš„ç»“æœ
    for algo in algorithms:
        try:
            # åŠ è½½å›ºå®šæ–‡ä»¶åçš„ç»“æœæ–‡ä»¶
            result_file = f"./results/{algo}/results.json"
            with open(result_file, 'r') as f:
                data = json.load(f)
                results[algo] = data
                print(f"{algo}è®­ç»ƒç»“æœ:")
                print(f"  è®­ç»ƒå›åˆæ•°: {data['num_episodes']}")
                print(f"  è®­ç»ƒæ—¶é—´: {data['training_time_seconds']:.2f}ç§’")
                print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {data['final_avg_reward']:.2f}")
                print(f"  æœ€ä½³å›åˆå¥–åŠ±: {data['best_reward']:.2f}")
                print(f"  æœ€å·®å›åˆå¥–åŠ±: {data['worst_reward']:.2f}")
                print("-" * 40)
        except Exception as e:
            print(f"æœªæ‰¾åˆ°{algo}ç»“æœæ–‡ä»¶æˆ–è¯»å–å¤±è´¥: {e}")
            results[algo] = None
    
    # ç»˜åˆ¶æ¯”è¾ƒå›¾
    valid_results = {k: v for k, v in results.items() if v is not None}
    if len(valid_results) >= 2:
        # åˆ›å»ºå›¾å½¢ï¼Œè°ƒæ•´å›¾å½¢å¤§å°å’Œå¸ƒå±€
        fig = plt.figure(figsize=(16, 12))
        
        # é¢œè‰²å’Œçº¿å‹
        colors = {'IPPO': 'blue', 'CPPO': 'red', 'MAPPO': 'green'}
        
        # 1. å¥–åŠ±æ›²çº¿å¯¹æ¯”
        ax1 = plt.subplot(3, 3, 1)
        for algo, data in valid_results.items():
            rewards = data['episode_rewards']
            if rewards:
                plt.plot(rewards, alpha=0.6, label=f'{algo}', color=colors.get(algo, 'black'))
        
        plt.xlabel('Episode')
        plt.ylabel('Reward')
        plt.title('Reward Curve Comparison')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 2. å¹³æ»‘å¥–åŠ±æ›²çº¿å¯¹æ¯”
        ax2 = plt.subplot(3, 3, 2)
        window = 20
        
        for algo, data in valid_results.items():
            rewards = data['episode_rewards']
            if rewards and len(rewards) >= window:
                smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')
                plt.plot(range(window-1, len(rewards)), smooth, 
                        label=f'{algo} (smoothed)', linewidth=2, color=colors.get(algo, 'black'))
        
        plt.xlabel('Episode')
        plt.ylabel('Smoothed Reward')
        plt.title(f'Smoothed Reward Comparison (window={window})')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. æœ€ç»ˆè¡¨ç°å¯¹æ¯”
        ax3 = plt.subplot(3, 3, 3)
        final_performance = []
        labels = []
        colors_list = []
        
        for algo, data in valid_results.items():
            final_avg = data['final_avg_reward']
            final_performance.append(final_avg)
            labels.append(algo)
            colors_list.append(colors.get(algo, 'gray'))
        
        if final_performance:
            bars = plt.bar(labels, final_performance, color=colors_list, alpha=0.7, edgecolor='black')
            plt.xlabel('Algorithm')
            plt.ylabel('Average Reward')
            plt.title('Final Performance Comparison')
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
            for bar, value in zip(bars, final_performance):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{value:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # 4. è®­ç»ƒæ—¶é—´å¯¹æ¯”
        ax4 = plt.subplot(3, 3, 4)
        training_times = []
        labels = []
        
        for algo, data in valid_results.items():
            training_times.append(data['training_time_seconds'])
            labels.append(algo)
        
        if training_times:
            bars = plt.bar(labels, training_times, color=colors_list, alpha=0.7, edgecolor='black')
            plt.xlabel('Algorithm')
            plt.ylabel('Training Time (seconds)')
            plt.title('Training Time Comparison')
            
            for bar, value in zip(bars, training_times):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{value:.1f}s', ha='center', va='bottom')
        
        # 5. å­¦ä¹ ç¨³å®šæ€§å¯¹æ¯”
        ax5 = plt.subplot(3, 3, 5)
        stabilities = []
        labels = []
        
        for algo, data in valid_results.items():
            rewards = data['episode_rewards']
            if rewards and len(rewards) >= 20:
                # è®¡ç®—æœ€å50%å›åˆçš„æ–¹å·®ï¼ˆé€†åºï¼Œæ–¹å·®è¶Šå°è¶Šç¨³å®šï¼‰
                half_len = len(rewards) // 2
                latter_half = rewards[half_len:]
                variance = np.var(latter_half)
                stabilities.append(1/(variance + 1e-6))  # ç¨³å®šæ€§æŒ‡æ ‡ï¼Œæ–¹å·®è¶Šå°å€¼è¶Šå¤§
                labels.append(algo)
        
        if stabilities:
            bars = plt.bar(labels, stabilities, color=colors_list, alpha=0.7, edgecolor='black')
            plt.xlabel('Algorithm')
            plt.ylabel('Stability (1/Variance)')
            plt.title('Learning Stability Comparison')
            
            for bar, value in zip(bars, stabilities):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{value:.2f}', ha='center', va='bottom')
        
        # 6. ç»¼åˆè¯„åˆ†é›·è¾¾å›¾
        ax6 = plt.subplot(3, 3, (6, 9), polar=True)  # å ç”¨ä¸¤ä¸ªä½ç½®ï¼Œä½¿é›·è¾¾å›¾æ›´å¤§
        
        if len(valid_results) >= 2:
            categories = ['Final Reward', 'Training Speed', 'Stability', 'Best Performance']
            N = len(categories)
            
            angles = [n / float(N) * 2 * np.pi for n in range(N)]
            angles += angles[:1]
            
            for algo, data in valid_results.items():
                # å½’ä¸€åŒ–è¯„åˆ† (0-1)
                final_score = min(1.0, data['final_avg_reward'] / 100)  # å‡è®¾æœ€å¤§å¥–åŠ±ä¸º100
                speed_score = min(1.0, 300 / max(data['training_time_seconds'], 1))  # å‡è®¾300ç§’ä¸ºåŸºå‡†
                
                # è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
                rewards = data['episode_rewards']
                if len(rewards) >= 20:
                    half_len = len(rewards) // 2
                    latter_half = rewards[half_len:]
                    stability = 1 - min(1.0, np.std(latter_half) / 50)  # å‡è®¾æ ‡å‡†å·®50ä¸ºæœ€å·®
                else:
                    stability = 0.5
                
                best_score = min(1.0, data['best_reward'] / 150)  # å‡è®¾æœ€ä½³å¥–åŠ±150ä¸ºæ»¡åˆ†
                
                values = [final_score, speed_score, stability, best_score]
                values += values[:1]
                
                plt.plot(angles, values, linewidth=2, label=algo, color=colors.get(algo, 'black'))
                plt.fill(angles, values, alpha=0.1)
            
            plt.xticks(angles[:-1], categories)
            plt.yticks([0.2, 0.4, 0.6, 0.8, 1.0], ['0.2', '0.4', '0.6', '0.8', '1.0'])
            plt.title('Comprehensive Ability Radar Chart')
            plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        
        # è®¾ç½®ä¸»æ ‡é¢˜å’Œè°ƒæ•´å¸ƒå±€
        plt.suptitle('Multi-Algorithm Comparison Analysis', fontsize=16, fontweight='bold')
        
        # è°ƒæ•´å­å›¾é—´è·ï¼Œå‡å°‘ä¸Šéƒ¨ç©ºç™½
        plt.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, 
                           wspace=0.3, hspace=0.4)
        
        plt.savefig("./results/algorithm_comparison.png", dpi=150, bbox_inches='tight')
        plt.show()
        
        # è¾“å‡ºè¯¦ç»†åˆ†æ
        print("\n" + "="*80)
        print("ç®—æ³•æ€§èƒ½è¯¦ç»†åˆ†æ")
        print("="*80)
        
        for algo, data in valid_results.items():
            rewards = data['episode_rewards']
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            final_avg = data['final_avg_reward']
            best_episode = data['best_reward']
            worst_episode = data['worst_reward']
            training_time = data['training_time_seconds']
            
            if rewards and len(rewards) >= 20:
                std_dev = np.std(rewards[-20:])
            else:
                std_dev = np.std(rewards) if rewards else 0
            
            print(f"\n{algo}ç®—æ³•:")
            print(f"  æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_avg:.2f}")
            print(f"  æœ€ä½³å›åˆå¥–åŠ±: {best_episode:.2f}")
            print(f"  æœ€å·®å›åˆå¥–åŠ±: {worst_episode:.2f}")
            print(f"  ç¨³å®šæ€§(æ ‡å‡†å·®): {std_dev:.2f}")
            print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
            
            # åˆ¤æ–­å­¦ä¹ è¶‹åŠ¿
            if len(rewards) >= 40:
                first_half = np.mean(rewards[:20])
                second_half = np.mean(rewards[-20:])
                improvement = second_half - first_half
                if improvement > 10:
                    trend = "æ˜¾è‘—ä¸Šå‡"
                elif improvement > 0:
                    trend = "ç¼“æ…¢ä¸Šå‡"
                elif improvement > -10:
                    trend = "åŸºæœ¬ç¨³å®š"
                else:
                    trend = "ä¸‹é™"
                print(f"  å­¦ä¹ è¶‹åŠ¿: {trend} (æ”¹å–„: {improvement:.2f})")
        
        # æ‰¾å‡ºæœ€ä½³ç®—æ³•
        best_algo = None
        best_score = -float('inf')
        
        for algo, data in valid_results.items():
            # ç»¼åˆè¯„åˆ†ï¼šæœ€ç»ˆè¡¨ç° * ç¨³å®šæ€§ / è®­ç»ƒæ—¶é—´
            final_avg = data['final_avg_reward']
            training_time = max(data['training_time_seconds'], 1)  # é¿å…é™¤ä»¥0
            
            # è®¡ç®—ç¨³å®šæ€§
            rewards = data['episode_rewards']
            if len(rewards) >= 20:
                half_len = len(rewards) // 2
                latter_half = rewards[half_len:]
                stability = 1 - min(1.0, np.std(latter_half) / 50)
            else:
                stability = 0.5
            
            score = final_avg * stability / training_time
            
            if score > best_score:
                best_score = score
                best_algo = algo
        
        if best_algo:
            print(f"\nğŸ† ç»¼åˆæœ€ä½³ç®—æ³•: {best_algo}")
            print(f"   ç»¼åˆè¯„åˆ†: {best_score:.4f}")
            print(f"   æœ€ç»ˆå¥–åŠ±: {valid_results[best_algo]['final_avg_reward']:.2f}")
            print(f"   è®­ç»ƒæ—¶é—´: {valid_results[best_algo]['training_time_seconds']:.2f}ç§’")
            
    return results

def train_all_algorithms(num_episodes=100):
    """è®­ç»ƒæ‰€æœ‰ä¸‰ç§ç®—æ³•å¹¶å¯¹æ¯”"""
    print("\n" + "="*80)
    print("è®­ç»ƒæ‰€æœ‰ä¸‰ç§ç®—æ³•: IPPO, CPPO, MAPPO")
    print("="*80)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("./results", exist_ok=True)
    
    # è®­ç»ƒä¸‰ç§ç®—æ³•
    all_results = {}
    
    algorithms = ["IPPO", "CPPO", "MAPPO"]
    
    for algo in algorithms:
        print(f"\nå¼€å§‹è®­ç»ƒ {algo} ç®—æ³•...")
        episode_rewards, avg_rewards, loss_history = train_algorithm(algo, num_episodes=num_episodes)
        all_results[algo] = episode_rewards
    
    # åˆ†æç»“æœ
    analyze_results(algorithms)
    
    # è¾“å‡ºå¤§ä½œä¸šå®Œæˆæ€»ç»“
    print("\n" + "="*80)
    print("\nğŸ“Š å¤§ä½œä¸šå®Œæˆæ€»ç»“")
    print("="*80)
    print(f"\nç»“æœä¿å­˜åœ¨: ./results/ ç›®å½•")
    print("\næ¯ä¸ªç®—æ³•åŒ…å«:")
    print("  - results.json: å®Œæ•´çš„è®­ç»ƒç»“æœæ•°æ®")
    print("  - training_results.png: è®­ç»ƒç»“æœå›¾è¡¨")
    print("\nå¯¹æ¯”åˆ†æåŒ…å«:")
    print("  - algorithm_comparison.png: å¤šç®—æ³•å¯¹æ¯”å›¾è¡¨")
    
    print("\nğŸ” å®éªŒç»“æœæ‘˜è¦:")
    
    for algo in algorithms:
        result_file = f"./results/{algo}/results.json"
        if os.path.exists(result_file):
            with open(result_file, 'r') as f:
                data = json.load(f)
                print(f"  {algo}:")
                print(f"    æœ€ç»ˆå¹³å‡å¥–åŠ±: {data['final_avg_reward']:.2f}")
                print(f"    è®­ç»ƒæ—¶é—´: {data['training_time_seconds']:.2f}ç§’")
                print(f"    æœ€ä½³å›åˆ: {data['best_reward']:.2f}")
    
    print("\nğŸ“ˆ è¿›ä¸€æ­¥æ”¹è¿›å»ºè®®:")
    print("1. è°ƒæ•´ç½‘ç»œç»“æ„ï¼ˆå±‚æ•°ã€ç¥ç»å…ƒæ•°é‡ï¼‰")
    print("2. ä¼˜åŒ–è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡ã€æŠ˜æ‰£å› å­ã€clipå‚æ•°ï¼‰")
    print("3. å¢åŠ æ™ºèƒ½ä½“é—´é€šä¿¡æœºåˆ¶")
    print("4. ä½¿ç”¨æ›´å¤æ‚çš„å¤šæ™ºèƒ½ä½“åœºæ™¯")
    print("5. å®ç°ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆReplay Bufferï¼‰")
    print("6. æ·»åŠ è¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰ç­–ç•¥")

    return all_results

def main():
    """ä¸»å‡½æ•°"""
    print("VMAS Balanceåœºæ™¯ - ç›´æ¥å®ç°MARLç®—æ³•")
    print("=" * 80)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("./results", exist_ok=True)
    
    print("\né€‰æ‹©æ“ä½œ:")
    print("1. è®­ç»ƒIPPOç®—æ³•")
    print("2. è®­ç»ƒCPPOç®—æ³•")
    print("3. è®­ç»ƒMAPPOç®—æ³•")
    print("4. è®­ç»ƒä¸‰ç§ç®—æ³•å¹¶å¯¹æ¯”")
    print("5. åˆ†æå·²æœ‰ç»“æœ")
    
    try:
        choice = int(input("è¯·è¾“å…¥é€‰æ‹© (1-5): "))
    except:
        choice = 4  # é»˜è®¤è®­ç»ƒæ‰€æœ‰ç®—æ³•å¹¶å¯¹æ¯”
    
    if choice == 1:
        print("\nè®­ç»ƒIPPOç®—æ³•...")
        episode_rewards, avg_rewards, loss_history = train_algorithm("IPPO", num_episodes=20000)
        
    elif choice == 2:
        print("\nè®­ç»ƒCPPOç®—æ³•...")
        episode_rewards, avg_rewards, loss_history = train_algorithm("CPPO", num_episodes=20000)
        
    elif choice == 3:
        print("\nè®­ç»ƒMAPPOç®—æ³•...")
        episode_rewards, avg_rewards, loss_history = train_algorithm("MAPPO", num_episodes=20000)
        
    elif choice == 4:
        print("\nè®­ç»ƒä¸‰ç§ç®—æ³•å¹¶å¯¹æ¯”...")
        train_all_algorithms(num_episodes=20000)
        
    elif choice == 5:
        print("\nåˆ†æå·²æœ‰ç»“æœ...")
        analyze_results()
        
    else:
        print("\næ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤è®­ç»ƒæ‰€æœ‰ç®—æ³•å¹¶å¯¹æ¯”")
        train_all_algorithms(num_episodes=20000)

if __name__ == "__main__":
    main() 